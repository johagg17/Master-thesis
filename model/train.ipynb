{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from model import *\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "       'Hello, how are you? I am Romeo.\\n'\n",
    "       'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "       'Nice meet you too. How are you today?\\n'\n",
    "       'Great. My baseball team won the competition.\\n'\n",
    "       'Oh Congratulations, Juliet\\n'\n",
    "       'Thanks you Romeo'\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    \n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "    \n",
    "    \n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(config):\n",
    "    batch_size = config['batch_size']\n",
    "    maxlen = config['maxlen']\n",
    "    max_pred = config['max_pred']\n",
    "    \n",
    "    \n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) \n",
    "\n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    " \n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "\n",
    "\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "       # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'wordvocab_size':vocab_size,\n",
    "    'maxlen': 64,\n",
    "    'batch_size': 32,\n",
    "    'max_pred': 5, \n",
    "    'n_layers': 12,\n",
    "    'n_heads': 12,\n",
    "    'd_model': 768,\n",
    "    'd_ff': 768 * 4,\n",
    "    'd_k': 64,\n",
    "    'n_segments': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['maxlen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordvocab_size': 29,\n",
       " 'maxlen': 64,\n",
       " 'batch_size': 32,\n",
       " 'max_pred': 5,\n",
       " 'n_layers': 12,\n",
       " 'n_heads': 12,\n",
       " 'd_model': 768,\n",
       " 'd_ff': 3072,\n",
       " 'd_k': 64,\n",
       " 'n_segments': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(config = config)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch = make_batch(config=config)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3, 15,  ...,  0,  0,  0],\n",
       "        [ 1, 20, 14,  ...,  0,  0,  0],\n",
       "        [ 1, 20, 14,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 16,  8,  ...,  0,  0,  0],\n",
       "        [ 1,  3, 22,  ...,  0,  0,  0],\n",
       "        [ 1, 16,  8,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14,  1,  4,  0,  0],\n",
       "        [ 8,  7,  0,  0,  0],\n",
       "        [ 3,  8,  0,  0,  0],\n",
       "        [ 3,  0,  0,  0,  0],\n",
       "        [11, 14, 16,  0,  0],\n",
       "        [10,  8,  0,  0,  0],\n",
       "        [10,  7, 11,  0,  0],\n",
       "        [15, 18,  4,  0,  0],\n",
       "        [18, 10,  7,  0,  0],\n",
       "        [ 6, 10,  0,  0,  0],\n",
       "        [ 1,  3,  0,  0,  0],\n",
       "        [12,  5,  6,  0,  0],\n",
       "        [ 7,  5,  0,  0,  0],\n",
       "        [ 3, 15,  7,  0,  0],\n",
       "        [ 7,  0,  0,  0,  0],\n",
       "        [10,  9, 12,  0,  0],\n",
       "        [ 6,  2,  9,  0,  0],\n",
       "        [16,  2,  7,  0,  0],\n",
       "        [16,  5,  3,  0,  0],\n",
       "        [ 2, 12,  6,  0,  0],\n",
       "        [ 6,  0,  0,  0,  0],\n",
       "        [10,  3,  0,  0,  0],\n",
       "        [11,  4,  0,  0,  0],\n",
       "        [ 1, 10, 14,  0,  0],\n",
       "        [ 2,  0,  0,  0,  0],\n",
       "        [ 7,  6, 14,  0,  0],\n",
       "        [ 7,  3,  5,  0,  0],\n",
       "        [ 6,  2,  0,  0,  0],\n",
       "        [ 7,  0,  0,  0,  0],\n",
       "        [13, 17, 18,  0,  0],\n",
       "        [14, 15,  1,  0,  0],\n",
       "        [ 3,  7,  4,  0,  0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost = 2.512177\n",
      "Epoch: 0020 cost = 0.176875\n",
      "Epoch: 0030 cost = 0.459661\n",
      "Epoch: 0040 cost = 1.172263\n",
      "Epoch: 0050 cost = 228.027298\n",
      "Epoch: 0060 cost = 1.047376\n",
      "Epoch: 0070 cost = 0.179516\n",
      "Epoch: 0080 cost = 11.528533\n",
      "Epoch: 0090 cost = 4.905905\n",
      "Epoch: 0100 cost = 2.647542\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "\n",
      "['[CLS]', '[MASK]', 'you', 'romeo', '[SEP]', 'hello', 'how', 'are', 'you', 'i', 'baseball', 'romeo', '[SEP]']\n",
      "masked tokens list :  [22, 12]\n",
      "predict masked tokens list :  [22, 22]\n",
      "isNext :  False\n",
      "predict isNext :  False\n"
     ]
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print()\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
