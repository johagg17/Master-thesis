{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa05d27-5a48-4923-8bbb-9ce00fda57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from utils.packages import *\n",
    "\n",
    "global_params = {\n",
    "    'max_seq_len': 64,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 10,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cuda' #change this to run on cuda #'cuda:0'\n",
    "}\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "stats_path = '../data/datasets/synthea/Smaller_cohorts/train_stats/'\n",
    "condfiles = {'dd':stats_path + 'dd_cond_probs.empirical.p', \n",
    "             'dp':stats_path + 'dp_cond_probs.empirical.p',\n",
    "             'dm':stats_path + 'dm_cond_probs.empirical.p',\n",
    "             'pp':stats_path + 'pp_cond_probs.empirical.p', \n",
    "             'pd':stats_path + 'pd_cond_probs.empirical.p',\n",
    "             'pm':stats_path + 'pd_cond_probs.empirical.p',\n",
    "             'mm':stats_path + 'mm_cond_probs.empirical.p', \n",
    "             'md':stats_path + 'md_cond_probs.empirical.p',\n",
    "             'mp':stats_path + 'mp_cond_probs.empirical.p',\n",
    "\n",
    "            }\n",
    "\n",
    "files = {'code':'../data/vocabularies/Synthea/Small_cohorts/diagnosiscodes.npy',\n",
    "         'age':'../data/vocabularies/Synthea/Small_cohorts/age.npy',\n",
    "         'labels':'../data/vocabularies/Synthea/Small_cohorts/labeldiagnosiscode.npy'\n",
    "        }\n",
    "tokenizer = EHRTokenizer(task='nextvisit', filenames=files)\n",
    "\n",
    "model_config = {\n",
    "        'vocab_size': len(tokenizer.getVoc('code').keys()), # number of disease + symbols for word embedding\n",
    "        'hidden_size': 288, #tune.choice([100, 150, 288]), #288, # word embedding and seg embedding hidden size\n",
    "        'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "        'age_vocab_size': len(tokenizer.getVoc('age').keys()), # number of vocab for age embedding,\n",
    "        'gender_vocab_size': 3,\n",
    "        'max_position_embeddings': train_params['max_len_seq'], # maximum number of tokens\n",
    "        'hidden_dropout_prob': 0.1, # dropout rate\n",
    "        'num_hidden_layers': 6, #4, # number of multi-head attention layers required\n",
    "        'num_attention_heads': 12, # number of attention heads\n",
    "        'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "        'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "        'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "        'initializer_range': 0.02, # parameter weight initializer range\n",
    "        'use_prior':False,\n",
    "        'reg':0.1,\n",
    "        'age':True,\n",
    "        'gender':False,\n",
    "        'epochs':20,\n",
    "    }\n",
    "conf = BertConfig(model_config)\n",
    "#train, val, test = train_test_val_split(dataset, train_ratio=0.6, validation_ratio=0.2, test_ratio=0.2)\n",
    "path='../data/datasets/synthea/Smaller_cohorts/'\n",
    "train = pd.read_parquet(path + 'train.parquet')\n",
    "val = pd.read_parquet(path + 'val.parquet')\n",
    "test = pd.read_parquet(path + 'test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac9ad17-291c-4948-99c9-ade6b637d04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "folderpath = '../data/pytorch_datasets/Synthea/Small_cohorts/'\n",
    "feature_types = {'diagnosis':True, 'medications':False, 'procedures':False}\n",
    "testd = EHRDatasetCodePrediction(test, max_len=train_params['max_len_seq'], tokenizer=tokenizer, feature_types=feature_types, save_folder=folderpath, conditional_files=condfiles, run_type='test_nextvisit')\n",
    "testloader = torch.utils.data.DataLoader(testd, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7275f67-b3e5-480b-ac14-d489a7b44808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
