{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb20c312-c197-4d47-a070-3419de302cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from utils.packages import *\n",
    "\n",
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "global_params = {\n",
    "    'max_seq_len': 64,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 10,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cuda' #change this to run on cuda #'cuda:0'\n",
    "}\n",
    "\n",
    "#train, val, test = train_test_val_split(dataset, train_ratio=0.6, validation_ratio=0.2, test_ratio=0.2)\n",
    "path='../data/datasets/synthea/Smaller_cohorts/'\n",
    "train = pd.read_parquet(path + 'train.parquet')\n",
    "val = pd.read_parquet(path + 'val.parquet')\n",
    "test = pd.read_parquet(path + 'test.parquet')\n",
    "\n",
    "stats_path = '../data/datasets/synthea/Smaller_cohorts/train_stats/'\n",
    "condfiles = {'dd':stats_path + 'dd_cond_probs.empirical.p', \n",
    "             'dp':stats_path + 'dp_cond_probs.empirical.p',\n",
    "             'dm':stats_path + 'dm_cond_probs.empirical.p',\n",
    "             'pp':stats_path + 'pp_cond_probs.empirical.p', \n",
    "             'pd':stats_path + 'pd_cond_probs.empirical.p',\n",
    "             'pm':stats_path + 'pd_cond_probs.empirical.p',\n",
    "             'mm':stats_path + 'mm_cond_probs.empirical.p', \n",
    "             'md':stats_path + 'md_cond_probs.empirical.p',\n",
    "             'mp':stats_path + 'mp_cond_probs.empirical.p',\n",
    "\n",
    "            }\n",
    "\n",
    "files = {'code':'../data/vocabularies/Synthea/Small_cohorts/diagnosiscodes.npy',\n",
    "         'age':'../data/vocabularies/Synthea/Small_cohorts/age.npy'\n",
    "        }\n",
    "\n",
    "feature_types = {'diagnosis':True, 'medications':False, 'procedures':False}\n",
    "tokenizer = EHRTokenizer(task='readmission', filenames=files)\n",
    "\n",
    "model_config = {\n",
    "        'vocab_size': len(tokenizer.getVoc('code').keys()), # number of disease + symbols for word embedding\n",
    "        'hidden_size': 288, #tune.choice([100, 150, 288]), #288, # word embedding and seg embedding hidden size\n",
    "        'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "        'age_vocab_size': len(tokenizer.getVoc('age').keys()), # number of vocab for age embedding,\n",
    "        'gender_vocab_size': 3,\n",
    "        'max_position_embeddings': train_params['max_len_seq'], # maximum number of tokens\n",
    "        'hidden_dropout_prob': 0.1, # dropout rate\n",
    "        'num_hidden_layers': 6, #4, # number of multi-head attention layers required\n",
    "        'num_attention_heads': 12, # number of attention heads\n",
    "        'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "        'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "        'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "        'initializer_range': 0.02, # parameter weight initializer range\n",
    "        'use_prior':False,\n",
    "        'reg':0.1,\n",
    "        'age':True,\n",
    "        'gender':False,\n",
    "        'epochs':20,\n",
    "    }\n",
    "\n",
    "folderpath = '../data/pytorch_datasets/Synthea/Small_cohorts/'\n",
    "testd = EHRDatasetReadmission(test, feature_types=feature_types, max_len=train_params['max_len_seq'], tokenizer=tokenizer, conditional_files=condfiles, save_folder=folderpath, run_type='test_readmission', nvisits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e75f0f-48d1-4634-bd42-7dce6fecc022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "testloader = torch.utils.data.DataLoader(testd, batch_size=32, shuffle=True, num_workers=4)\n",
    "conf = BertConfig(model_config)\n",
    "model = BertSinglePrediction(conf, num_labels=1) \n",
    "PATH = \"../saved_models/MLM/BEHRT_small_cohorts_synthea\"\n",
    "model = load_model(PATH, model)\n",
    "params = list(model.named_parameters())\n",
    "optim = adam(params, optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70741d71-62ef-4e7f-ac87-f97a3dabc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.Tensor()\n",
    "adm_labels = []\n",
    "\n",
    "for idx, batch in enumerate(testloader):\n",
    "    age, gender, code, position, segment, mask, label, prior_guide = batch\n",
    "    #sentence = tokenizer.convert_tokens_to_ids(code, 'code')\n",
    "    adm_labels.extend(label[:, 0].tolist())\n",
    "    loss, preds, labels, attention_scores, poolout = model(code, age_ids=age, gender_ids=gender, seg_ids=segment, posi_ids=position, attention_mask=mask, labels=label, prior_guide=prior_guide)\n",
    "    \n",
    "    seq = torch.cat((seq, poolout), 0)\n",
    "    \n",
    "    if idx == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4eeff3-c591-4af5-a4c8-1410ef42efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10)) #each subplot of size 6x6, each row will hold 4 plots\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "sequence_embed_reduced_embeds = tsne.fit_transform(seq.detach().numpy())\n",
    "df = pd.DataFrame.from_dict({'x':sequence_embed_reduced_embeds[:,0],'y':sequence_embed_reduced_embeds[:,1], 'label':adm_labels})\n",
    "\n",
    "sns.scatterplot(data=df,x='x',y='y', hue='label',ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
