{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbfdc86-31a5-4a34-971b-2fac1a1feb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from utils.packages import *\n",
    "\n",
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "global_params = {\n",
    "    'max_seq_len': 64,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 10,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cuda' #change this to run on cuda #'cuda:0'\n",
    "}\n",
    "\n",
    "files = {'code':'../data/vocabularies/Synthea/snomedrxnorm.npy',\n",
    "         'age':'../data/vocabularies/Synthea/age.npy'\n",
    "        }\n",
    "tokenizer = EHRTokenizer(task='MLM', filenames=files)\n",
    "\n",
    "model_config = {\n",
    "        'vocab_size': len(tokenizer.getVoc('code').keys()), # number of disease + symbols for word embedding\n",
    "        'hidden_size': 288, #tune.choice([100, 150, 288]), #288, # word embedding and seg embedding hidden size\n",
    "        'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "        'age_vocab_size': len(tokenizer.getVoc('age').keys()), # number of vocab for age embedding,\n",
    "        'gender_vocab_size': 3,\n",
    "        'max_position_embeddings': train_params['max_len_seq'], # maximum number of tokens\n",
    "        'hidden_dropout_prob': 0.1, # dropout rate\n",
    "        'num_hidden_layers': 6, #4, # number of multi-head attention layers required\n",
    "        'num_attention_heads': 12, # number of attention heads\n",
    "        'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "        'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "        'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "        'initializer_range': 0.02, # parameter weight initializer range\n",
    "        'use_prior':False,\n",
    "        'reg':0.1,\n",
    "        'age':True,\n",
    "        'gender':False,\n",
    "        'epochs':20,\n",
    "    }\n",
    "\n",
    "stats_path = '../data/datasets/synthea/Smaller_cohorts/train_stats/'\n",
    "condfiles = {'dd':stats_path + 'dd_cond_probs.empirical.p', \n",
    "             'dp':stats_path + 'dp_cond_probs.empirical.p',\n",
    "             'dm':stats_path + 'dm_cond_probs.empirical.p',\n",
    "             'pp':stats_path + 'pp_cond_probs.empirical.p', \n",
    "             'pd':stats_path + 'pd_cond_probs.empirical.p',\n",
    "             'pm':stats_path + 'pd_cond_probs.empirical.p',\n",
    "             'mm':stats_path + 'mm_cond_probs.empirical.p', \n",
    "             'md':stats_path + 'md_cond_probs.empirical.p',\n",
    "             'mp':stats_path + 'mp_cond_probs.empirical.p',\n",
    "\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ff7094-e998-4099-9c22-22ba1e1d9ae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'save_folder' and 'feature_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0f2f781bfd79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtraind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEHRDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_len_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditional_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'save_folder' and 'feature_types'"
     ]
    }
   ],
   "source": [
    "#train, val, test = train_test_val_split(dataset, train_ratio=0.6, validation_ratio=0.2, test_ratio=0.2)\n",
    "path='../data/datasets/synthea/Smaller_cohorts/'\n",
    "train = pd.read_parquet(path + 'train.parquet')\n",
    "val = pd.read_parquet(path + 'val.parquet')\n",
    "test = pd.read_parquet(path + 'test.parquet')\n",
    "\n",
    "traind = EHRDataset(train, max_len=train_params['max_len_seq'], conditional_files=condfiles, tokenizer=tokenizer)\n",
    "trainloader = torch.utils.data.DataLoader(traind, batch_size=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ada94-bca7-477f-83b7-722ca9e1a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n",
    "PATH = \"../saved_models/MLM/model_with_prior_82test\"\n",
    "model = load_model(PATH, model)\n",
    "params = list(model.named_parameters())\n",
    "optim = adam(params, optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2cd986-52a9-4392-9a99-8bd33c88671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_output = []\n",
    "masks = torch.Tensor()\n",
    "seq = torch.Tensor()\n",
    "\n",
    "sentences = []\n",
    "for idx, batch in enumerate(trainloader):\n",
    "    age, gender, code, position, segment, mask, label, prior_guide = batch\n",
    "    #sentence = tokenizer.convert_tokens_to_ids(code, 'code')\n",
    "    loss, preds, labels, attention_scores, sequence_outputs = model(code, age_ids=age, gender_ids=gender, seg_ids=segment, posi_ids=position, attention_mask=mask, labels=label, prior_guide=prior_guide)\n",
    "    #age_embedding = embedding_output[2]\n",
    "    #disease_embed = embedding_output[1]\n",
    "    #gender_embed = embedding_output[3]\n",
    "    \n",
    "    sentence = tokenizer.convert_ids_to_tokens(code.numpy().squeeze(), 'code')\n",
    "    sentences.extend(sentence)\n",
    "    seq_out = sequence_outputs[-1]\n",
    "    \n",
    "    seq = torch.cat((seq, seq_out), 0)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34210b55-7a66-40e2-883a-68a1daa98d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = seq.reshape(seq.shape[0] *  seq.shape[1], -1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1df6e3-7f23-4e56-b3e9-a12ec76b68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TSNE(n_components=2)\n",
    "X_transformed = pca.fit_transform(s)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10)) #each subplot of size 6x6, each row will hold 4 plots\n",
    "df = pd.DataFrame.from_dict({'x':X_transformed[:,0],'y':X_transformed[:,1], 'labels':sentences})\n",
    "sns.scatterplot(data=df,x='x',y='y', hue='labels',ax=ax)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3),ncol=3, fancybox=True, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f64f60-f58b-42b5-8273-34baa8bdd9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
