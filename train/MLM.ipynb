{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b5e6d03-c6a6-4c47-89c7-8238e87237ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bba6d229-2037-43e0-8551-fdf1d53e7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59a27ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import EHRDataset\n",
    "from model.tokenizer import EHRTokenizer\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from torch.utils.data import DataLoader\n",
    "from model.model2 import *\n",
    "#from utils.config import BertConfig\n",
    "from model.trainer import PatientTrajectoryPredictor\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "053ea6ff-ed4c-49e3-9517-2f47795b5a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b75076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f7974f8-62e3-4039-adc2-a8ddf30786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f185f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(params, config=None):\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'lr': 3e-5,\n",
    "            'warmup_proportion': 0.1,\n",
    "            'weight_decay': 0.01\n",
    "        }\n",
    "        \n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0}\n",
    "    ]\n",
    "\n",
    "    optim = Bert.optimization.BertAdam(optimizer_grouped_parameters,\n",
    "                                       lr=config['lr'],\n",
    "                                       warmup=config['warmup_proportion'])\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c177dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'C:/Users/Johan/Documents/Skola/MasterThesis/Master-thesis/pre-processing/combined-csv-files.csv'\n",
    "path = '../processing/readmission_data_ccsr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce83c0-79b8-4467-9fe7-59ea8085ae70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "835899dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "732f78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 32,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cuda' #change this to run on cuda #'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "456de5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0345b2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>label</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>ccsr</th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>tobacco_abuse</th>\n",
       "      <th>ndc</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10008245</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[S32391A, E870, F72, G40909, R509, J45909, M8...</td>\n",
       "      <td>[[INJ003, END011, MBD014, NVS009, SYM002, RSP0...</td>\n",
       "      <td>[47.0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[69543013120, 904645561, 54482014407, 6655300...</td>\n",
       "      <td>[26674944]</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10031358</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[E11618, M86179, I96, E871, L97509, N179, E11...</td>\n",
       "      <td>[[END003, MUS002, CIR028, END011, SKN004, GEN0...</td>\n",
       "      <td>[62.0, 63.0, 64.0, 64.0, 65.0, 65.0]</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[63323026201, 71015892, 51079045420, 74706811...</td>\n",
       "      <td>[27421511, 28279098, 24522342, 29498981, 29887...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id               label  \\\n",
       "0    10008245                 [0]   \n",
       "1    10031358  [0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                            icd_code  \\\n",
       "0  [[S32391A, E870, F72, G40909, R509, J45909, M8...   \n",
       "1  [[E11618, M86179, I96, E871, L97509, N179, E11...   \n",
       "\n",
       "                                                ccsr  \\\n",
       "0  [[INJ003, END011, MBD014, NVS009, SYM002, RSP0...   \n",
       "1  [[END003, MUS002, CIR028, END011, SKN004, GEN0...   \n",
       "\n",
       "                                    age       alcohol_abuse  \\\n",
       "0                                [47.0]                 [0]   \n",
       "1  [62.0, 63.0, 64.0, 64.0, 65.0, 65.0]  [1, 1, 0, 0, 0, 0]   \n",
       "\n",
       "        tobacco_abuse                                                ndc  \\\n",
       "0                 [0]  [[69543013120, 904645561, 54482014407, 6655300...   \n",
       "1  [0, 0, 0, 0, 0, 0]  [[63323026201, 71015892, 51079045420, 74706811...   \n",
       "\n",
       "                                             hadm_id gender  \n",
       "0                                         [26674944]      M  \n",
       "1  [27421511, 28279098, 24522342, 29498981, 29887...      M  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06173b16-e355-4072-b2d2-16545a386323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170296"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be625ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EHRTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d869c1d-64fb-4731-a693-a91a6aceb9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.getVoc('code').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec6d8921-f627-4e78-85c6-7ae56f9d3f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.getVoc('age').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f01e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(tokenizer.getVoc('code').keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(tokenizer.getVoc('age').keys()), # number of vocab for age embedding,\n",
    "    #'gender_vocab_size': 3,\n",
    "    'max_position_embeddings': train_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 2, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 4, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 768, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c869175-5363-41f5-8e69-0ce8aca95579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "914c04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcd16dcb-5eaf-4a46-8db9-445f4dce0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboarddir = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f0a7a73-ec91-4ba5-aff2-b6eeb13dd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold=KFold(n_splits=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76dfd77e-0e65-43a8-bb34-d05555b6606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allacc = []\n",
    "#dataset = EHRDataset(data, max_len=train_params['max_len_seq'], tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4a0b4a3-d2f2-4eca-aab3-f0dc183f358a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor fold,(train_idx,test_idx) in enumerate(kfold.split(data)):\\n    print(\\'------------fold no---------{}----------------------\\'.format(fold))\\n    \\n    trainer = pl.Trainer(\\n            max_epochs=50, \\n            gpus=-1,\\n            logger=pl.loggers.TensorBoardLogger(save_dir=tensorboarddir),\\n            callbacks=[pl.callbacks.progress.TQDMProgressBar()], \\n            progress_bar_refresh_rate=1,\\n            weights_summary=None, # Can be None, top or full\\n            num_sanity_val_steps=10,\\n        )\\n    \\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\\n    test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\\n    \\n    trainloader = torch.utils.data.DataLoader(dataset, batch_size=train_params[\\'batch_size\\'], sampler=train_subsampler)\\n    testloader = torch.utils.data.DataLoader(dataset, batch_size=train_params[\\'batch_size\\'], sampler=test_subsampler)\\n    \\n    \\n    model = BertForMaskedLM(conf) #BertForMaskedLM(conf)\\n    params = list(model.named_parameters())\\n    optim = adam(params, optim_param)\\n    \\n    patienttrajectory = PatientTrajectoryPredictor(model, optim, optim_param, metrics=True)\\n    \\n    trainer.fit(\\n        patienttrajectory, \\n        train_dataloader=trainloader,\\n    );\\n  \\n    predictions = trainer.predict(patienttrajectory, dataloaders=testloader)\\n    avg_acc = sum([ stats[\\'Accuracy\\'] for stats in predictions ]) / len(testloader)\\n    allacc.append(avg_acc)\\n    print(\"Average Test accuracy for fold {}: {} \".format(fold, avg_acc))\\n    break\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for the crossval, might not be runnable since we have done some debugging\n",
    "\n",
    "'''\n",
    "for fold,(train_idx,test_idx) in enumerate(kfold.split(data)):\n",
    "    print('------------fold no---------{}----------------------'.format(fold))\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "            max_epochs=50, \n",
    "            gpus=-1,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=tensorboarddir),\n",
    "            callbacks=[pl.callbacks.progress.TQDMProgressBar()], \n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,\n",
    "        )\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=train_params['batch_size'], sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(dataset, batch_size=train_params['batch_size'], sampler=test_subsampler)\n",
    "    \n",
    "    \n",
    "    model = BertForMaskedLM(conf) #BertForMaskedLM(conf)\n",
    "    params = list(model.named_parameters())\n",
    "    optim = adam(params, optim_param)\n",
    "    \n",
    "    patienttrajectory = PatientTrajectoryPredictor(model, optim, optim_param, metrics=True)\n",
    "    \n",
    "    trainer.fit(\n",
    "        patienttrajectory, \n",
    "        train_dataloader=trainloader,\n",
    "    );\n",
    "  \n",
    "    predictions = trainer.predict(patienttrajectory, dataloaders=testloader)\n",
    "    avg_acc = sum([ stats['Accuracy'] for stats in predictions ]) / len(testloader)\n",
    "    allacc.append(avg_acc)\n",
    "    print(\"Average Test accuracy for fold {}: {} \".format(fold, avg_acc))\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c02ae1-a2db-4688-a280-7349eed0e58d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing with BEHRTs training approach (without pytorch lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07bedfb1-7994-4e69-8f7b-9592a03fc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db35e7f0-59bd-47c4-907f-a1cdea876acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(label, pred):\n",
    "    logs = nn.LogSoftmax()\n",
    "    label=label.cpu().numpy()\n",
    "    ind = np.where(label!=-1)[0]\n",
    "    truepred = pred.detach().cpu().numpy()\n",
    "    truepred = truepred[ind]\n",
    "    truelabel = label[ind]\n",
    "    truepred = logs(torch.tensor(truepred))\n",
    "    outs = [np.argmax(pred_x) for pred_x in truepred.numpy()]\n",
    "    \n",
    "   # print(\"Truelabel:\")\n",
    "   # print(truelabel)\n",
    "    \n",
    "   # print(\"Output:\")\n",
    "   # print(outs)\n",
    "    precision = skm.precision_score(truelabel, outs, average='micro')\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea42dc98-e930-4cfc-9fa3-02685e197ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf) \n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "455eeada-6cfe-4898-a14f-f5f91badfc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "optim = adam(params, optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f036d132-371f-452b-8d0b-8c7a9a433a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, loader):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt= 0\n",
    "    start = time.time()\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        cnt +=1\n",
    "        batch = tuple(t.to(train_params['device']) for t in batch)\n",
    "        age_ids, gender_ids, input_ids, posi_ids, segment_ids, attMask, masked_label, _ = batch\n",
    "        #print()\n",
    "        loss, pred, label = model(input_ids, age_ids, gender_ids, segment_ids, posi_ids,attention_mask=attMask, labels=masked_label)\n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 200==0:\n",
    "            print(\"epoch: {}\\t| cnt: {}\\t|Loss: {}\\t| precision: {:.4f}\\t| time: {:.2f}\".format(e, cnt, temp_loss/2000, cal_acc(label, pred), time.time()-start))\n",
    "            temp_loss = 0\n",
    "            start = time.time()\n",
    "            \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "   #print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    #model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "    #create_folder(file_config['model_path'])\n",
    "    #output_model_file = os.path.join(file_config['model_path'], file_config['model_name'])\n",
    "\n",
    "    #torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        \n",
    "    cost = time.time() - start\n",
    "    return tr_loss, cost\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bab456b-d788-49db-94a5-d906169605d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e1198cf-4a54-407c-982e-857be9056627",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = EHRDataset(trainset, max_len=train_params['max_len_seq'], tokenizer=tokenizer)\n",
    "trainload = DataLoader(dataset=trainset, batch_size=train_params['batch_size'], shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "359c5a5f-7142-44d5-b376-3eac2d9b2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e17cb07-a7e5-4685-9553-ae8473d74c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| cnt: 1\t|Loss: 0.0031055378913879394\t| precision: 0.0068\t| time: 0.26\n",
      "epoch: 0\t| cnt: 201\t|Loss: 0.46001285314559937\t| precision: 0.1975\t| time: 6.50\n",
      "epoch: 0\t| cnt: 401\t|Loss: 0.4322800164222717\t| precision: 0.2097\t| time: 6.51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1950268/330654850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimecost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss after epoch {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1950268/3389936152.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, loader)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    loss, timecost = train(e, trainload)\n",
    "    loss = loss / len(trainload)\n",
    "    \n",
    "    print(\"Loss after epoch {}: {}\".format(e, loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e98b64-99d9-47cd-b236-a4b5f7a0f40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc6c58-2a60-4890-83c3-e16e70d4a337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
