{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5e6d03-c6a6-4c47-89c7-8238e87237ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba6d229-2037-43e0-8551-fdf1d53e7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a27ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/home/linler17/.conda/envs/masterenv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c177dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_patients = '../data/datasets/readmission_data_synthea'\n",
    "#prior1 = pd.read_parquet('../data/datasets/prior_table_1')\n",
    "#prior2 = pd.read_parquet('../data/datasets/prior_table_2')\n",
    "#priordataset = pd.concat([prior1, prior2])\n",
    "#data = pd.read_parquet(path_patients)\n",
    "#dataset = data.merge(priordataset, on='subject_id', how='inner')\n",
    "dataset = pd.read_parquet(path_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732f78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 32,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 32,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cuda' #change this to run on cuda #'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07e623b-c285-4ab2-8e76-a17334aa3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnos_codes = dataset['diagnos_code'].tolist()\n",
    "#med_codes = dataset['medication_code'].tolist()\n",
    "#diagnos_codes.extend(med_codes)\n",
    "#file_path = '../data/vocabularies/Synthea/snomedrxnorm'\n",
    "#write_codes_to_file(diagnos_codes, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7246e1f0-6d9e-4b6f-8775-fa46537ba62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ages = dataset['age'].tolist()\n",
    "#file_path = '../data/vocabularies/Synthea/age'\n",
    "#write_age_to_file(ages, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be625ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {'code':'../data/vocabularies/Synthea/snomedrxnorm.npy',\n",
    "         'age':'../data/vocabularies/Synthea/age.npy'\n",
    "        }\n",
    "tokenizer = EHRTokenizer(task='MLM', filenames=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173d5964-f294-4466-84da-b96f188a9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, val, test = train_test_val_split(dataset, train_ratio=0.6, validation_ratio=0.2, test_ratio=0.2)\n",
    "path='../data/datasets/synthea/'\n",
    "#train.to_parquet(path + 'train')\n",
    "#val.to_parquet(path + 'val')\n",
    "#test.to_parquet(path + 'test')\n",
    "\n",
    "train = pd.read_parquet(path + 'train')\n",
    "val = pd.read_parquet(path + 'val')\n",
    "test = pd.read_parquet(path + 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d654c3-1f2d-4bb3-bc5d-dad78dc19440",
   "metadata": {},
   "outputs": [],
   "source": [
    "condfiles = {'dd':'../data/train_stats/dd_cond_probs.empirical.p', \n",
    "             'dp':'../data/train_stats/dp_cond_probs.empirical.p', \n",
    "             'pp':'../data/train_stats/pp_cond_probs.empirical.p' , \n",
    "             'pd':'../data/train_stats/pd_cond_probs.empirical.p'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eafbb27-eeff-4b05-bb19-659723d0e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data\n",
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "traind = EHRDataset(train, max_len=train_params['max_len_seq'], conditional_files=condfiles, tokenizer=tokenizer)\n",
    "vald = EHRDataset(val, max_len=train_params['max_len_seq'], tokenizer=tokenizer, conditional_files=condfiles, run_type='val')\n",
    "testd = EHRDataset(test, max_len=train_params['max_len_seq'], tokenizer=tokenizer, conditional_files=condfiles, run_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f01e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(tokenizer.getVoc('code').keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, #tune.choice([100, 150, 288]), #288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(tokenizer.getVoc('age').keys()), # number of vocab for age embedding,\n",
    "    'gender_vocab_size': 3,\n",
    "    'max_position_embeddings': train_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 4, #4, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 300, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "    'use_prior':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3780d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboarddir = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec11dba-0dc8-45e3-a8b9-12c03d93005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(traind, batch_size=32, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(vald, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testd, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a91333fa-41d1-4210-ad79-130cebd00f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "#from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c82bfd1-971f-467a-8989-9f30f1cbc8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.10'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "753ac9ca-4056-4093-b491-7049d6de727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model_config, epochs, tensorboarddir, save_model=False):\n",
    "    \n",
    "    '''\n",
    "    callback = TuneReportCallback(\n",
    "    {\n",
    "        \"loss\": \"val_loss\",\n",
    "        \"mean_accuracy\": \"val_accuracy\"\n",
    "    },\n",
    "    on=\"validation_step\")\n",
    "    '''\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "            max_epochs=epochs, \n",
    "            gpus=2,\n",
    "            accelerator=\"ddp\",\n",
    "            #logger=pl.loggers.TensorBoardLogger(save_dir=tensorboarddir),\n",
    "            callbacks=[pl.callbacks.TQDMProgressBar()], #progress.ProgressBar()], \n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,\n",
    "        )\n",
    "    \n",
    "    conf = BertConfig(model_config)\n",
    "    model = BertForMaskedLM(conf) #BertForMaskedLM(conf)\n",
    "    params = list(model.named_parameters())\n",
    "    optim = adam(params, optim_param)\n",
    "    \n",
    "    patienttrajectory = TrainerMLM(model, optim, optim_param, 0.1, use_prior=model_config['use_prior'])\n",
    "    print(\"Trainer is fitting\")\n",
    "    trainer.fit(\n",
    "        patienttrajectory, \n",
    "        train_dataloader=trainloader,\n",
    "        val_dataloaders=valloader,\n",
    "    );\n",
    "    print(\"Predicting on test data\")\n",
    "    predictions = trainer.predict(patienttrajectory, dataloaders=testloader)\n",
    "    \n",
    "    avg_acc = sum([ stats['precision'] for stats in predictions ]) / len(predictions)\n",
    "    avg_acc*100\n",
    "    \n",
    "    #tune.report(mean_accuracy=avg_acc)\n",
    "    print(\"Avg precision score: {}\".format(avg_acc))\n",
    "    \n",
    "    if save_model:\n",
    "        print(\"Saving model\")\n",
    "        PATH = '../saved_models/MLM/model_without_prior'\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91a36623-41e0-4960-9bff-d2cfbc585d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-da133b4d4863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboarddir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorboarddir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-53b9c49caed8>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[0;34m(model_config, epochs, tensorboarddir, save_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprogress_bar_refresh_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mweights_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Can be None, top or full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mnum_sanity_val_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/masterenv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/masterenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mamp_backend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mamp_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mplugins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         )\n\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoggerConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_gpu_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/masterenv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_distributed_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_given_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/masterenv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36mset_distributed_mode\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m# finished configuring self._distrib_type, check ipython environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_interactive_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# for DDP overwrite nb processes by requested GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/masterenv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36mcheck_interactive_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_IS_INTERACTIVE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distrib_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distrib_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m             raise MisconfigurationException(\n\u001b[0;32m--> 944\u001b[0;31m                 \u001b[0;34mf\"`Trainer(strategy={self._distrib_type.value!r})` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0;34mf\" `Trainer(accelerator={self._distrib_type.value!r})` is not compatible with an interactive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;34m\" environment. Run your code as a script, or choose one of the compatible backends:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "train_test_model(model_config=model_config, epochs=5, tensorboarddir=tensorboarddir, save_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2cf4a-c133-4283-a666-d9e81cc188a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualize latent-space, namely the encoded space for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0d527d5-4da2-4ff8-9e07-f3addd5c093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM(conf) \n",
    "PATH = \"../saved_models/MLM/model_with_prior_82test\"\n",
    "model = load_model(PATH, model)\n",
    "params = list(model.named_parameters())\n",
    "optim = adam(params, optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d302fbdb-71fc-41b4-9b78-270bb9083c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsequence = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae9a220-4f78-4656-843e-a5f9ad3dc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentionprobs = get_attention_scores(model, testsequence, 1, tokenizer, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d94d5-7711-42a3-ab15-55c33c81ecc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
